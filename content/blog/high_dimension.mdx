---
title: 'Musings on High-Dimensional Spaces'
description: 'In high dimensional spaces, local minima are rare, but common in high-dimensional parametrizations of low-dimensional spaces'
image: '/images/blog/dimensions/dims1.png'
date: '2025-07-10'
tags: ['AI', 'SLT']
---

> In high dimensional spaces, local minima are rare, but common in high-dimensional parametrizations of low-dimensional spaces.

Look, I've always been fascinated with the dynamics of gradient descent and high dimensional spaces. I've studied this problem a bit (see my Bachelor's thesis [here](https://joanvelja.vercel.app/pdf/BSc_Thesis.pdf)) and read quite a lot of papers on the topic. All the fuss about scaling? I've internalized it, I reckon, by now. Yet when we talk about it, the simplistic claim being made is that bigger is better. There's a less simplistic, possibly more mathematically grounded claim-while still not galaxy-brain-that the dynamics of gradient descent in high-dimensional spaces are completely different from the dynamics in low-dimensional spaces. This blogpost is an attempt to back this claim with some actual math.


## Gradient Descent: a 19th-century stunt  

Ah, gradient descent. What an algorithm. So ridiculously simple, yet so powerful it's almost insulting. Augustin-Louis Cauchy (yeah, the limits theorem guy) figured this out way back in October 1847. He introduced the method in his paper "Méthode générale pour la résolution des systèmes d'équations simultanées" (General method for solving systems of simultaneous equations).<MarginNote label="Historical Aside">While the 1847 paper is the definitive, well-cited source, there's evidence that Cauchy had been working on the concepts behind steepest descent for a while. Earlier Cauchy papers from the 1820s use the expression "steepest descent", but in a *different* sense (contour deformation for asymptotic integrals, and derivative-based root-refinement). Interesting to note that Cauchy described the method in an equivalent form before the term "gradient" was even in common use. He worked with partial derivatives to iteratively find the minimum of a function.</MarginNote>

<center>
$$
\mathbf w_{t+1} = \mathbf w_t \;-\; \eta\,\nabla_{\!\mathbf w} L(\mathbf w_t)
$$
</center>

Cauchy's original algorithm was specifically for solving systems of linear equations, though he ended up presenting this new angle on how to iteratively move in the direction of the steepest descent of a function to find its minimum. Fast-forward to modern deep learning: we drop this 178-year-old routine into a loss landscape with **billions of coordinates** and expect it to come home with a global-ish minimum.

In my opinion, the most fascinating insight for why such an "old" algorithm (by today's standards) is still so ridiculously powerful is that gradient descent succeeds because it exploits a timeless mathematical truth: *local linear approximation is surprisingly effective in high-dimensional spaces*.

Will you look at the paradox? Neural networks create loss landscapes that are incredibly complex: imagine trying to navigate a terrain with millions or billions of dimensions, filled with saddle points, local minima, and seemingly chaotic structure. You'd think such a simple algorithm (essentially "follow the steepest downhill path") would get hopelessly lost.

But here's the thing: in very high-dimensional spaces, the curse of dimensionality actually becomes a blessing. Most critical points in these landscapes are saddle points rather than local minima, and gradient descent has a natural tendency to escape saddle points. The algorithm that Cauchy designed for astronomy calculations turns out to be perfectly suited for navigating the geometry of high-dimensional optimization.<MarginNote>There's also something beautiful about how gradient descent embodies the principle of **local information leading to global progress**. The algorithm never "sees" the entire loss landscape-it only knows the local slope at its current position. Yet this myopic view, when applied iteratively, can navigate incredibly complex spaces and find solutions that generalize across vast datasets.</MarginNote>

The miracle, effectively, comes in two parts.

1. **Saddle-point deserts**: In very high dimensions, true minima/maxima are rare; almost every critical point is a saddle. Dauphin et al. (2014) measured this directly and showed that SGD’s noisy steps slip past those saddles instead of getting marooned on them <Citation author="Dauphin et al." year="2014" title="Identifying and attacking the saddle point problem in high-dimensional non-convex optimization" url="https://arxiv.org/abs/1406.2572" venue="NIPS" />.

2. **From Saddles to Canyons**: Okay, so the landscape is mostly saddles. This helps SGD not get stuck, but it also paints a bleak picture of a universe with no good destinations. This is where the second part of the miracle kicks in. It would be quite reductive to label a neural network's loss function as a generic high-dimensional function; in a sense, it'd be more correct to call it a high-dimensional parameterization of a low-dimensional task.
This structure forces the landscape to be... well, redundant. There are countless ways to configure the weights to get the same result. When Sagun et al. (2017), among others, looked at the Hessian to map out this landscape, they found the Hessian's eigenvalues not to be random at all. Said eigenvalues split cleanly into two groups: a few large eigenvalues that define the 'hard' parts of the problem, and a giant cluster of eigenvalues right at zero<Citation author="Sagun et al." year="2017" title="Empirical Analysis of the Hessian of Over-Parametrized Neural Networks" url="https://arxiv.org/abs/1706.04454" venue="ICLR" />. This "bulk at zero" is the mathematical echo of the redundancy, revealing vast, flat valleys in parameter space.

<ImageThemeAdjuster
  src="/images/blog/dimensions/bottou.png"
  alt="Eigenvalues"
  strategy="invert"
  aspectRatio="16/9"
  className="w-full my-6"
  showCaption={true}
  caption="Source: Empirical Analysis of the Hessian of Over-Parametrized Neural Networks (2017).
  Ordered eigenvalues at a random initial point, and at the final point of GD. x-axis is the rank of the eigenvalue, and y-axis the value of the eigenvalue."
/>

<MarginNote>Think of it this way: instead of searching for a single magic bullet (a point-like minimum), the optimizer is tasked with finding a huge, flat valley full of good-enough solutions. The noise in SGD helps it settle into the flattest, widest parts of these valleys, which often lead to better generalization.</MarginNote>

Together, these two facts flip the usual “curse of dimensionality” into a blessing:  
*Escaping saddles is easy, and landing in something that **acts** like a minimum is statistically likely once you over-parameterise.*

## An Intuitive Picture

Let's build some intuition for why this is the case.

In a **1-dimensional** space, things are dead simple. Any point with a zero gradient is either a local minimum (the bottom of a valley) or a local maximum (the top of a hill). There's no in-between.

<ImageThemeAdjuster
  src="/images/blog/dimensions/1d_landscape.png"
  alt="1D Landscape"
  strategy="invert"
  aspectRatio="16/9"
  className="w-full my-6"
  showCaption={true}
  caption="1D loss landscape with minima and maxima."
/>

Now, let's move to **2 dimensions**. For a point to be a critical point, the gradient must be zero along both axes. If we assume the curvature along each axis is independent, we have four possibilities for the type of critical point: (min, min), (min, max), (max, min), and (max, max). Only the (min, min) case is a true local minimum, where the surface curves up along both axes. The (max, max) is a local maximum. The two mixed cases, (min, max) and (max, min), are saddle points. Already, we see that saddle points are as common as local minima and maxima combined.

<ImageThemeAdjuster
  src="/images/blog/dimensions/2d_landscape.png"
  alt="2D Landscape"
  strategy="invert"
  aspectRatio="16/9"
  className="w-full my-6"
  showCaption={true}
  caption="2D loss landscape with minima, maxima, and saddle points."
/>

Extrapolating this, for a point in an **N-dimensional** space to be a local minimum, it must be a minimum along *all N dimensions simultaneously*. If you think of the curvature in each dimension as an independent coin flip (either 'up' for a minimum or 'down' for a maximum), the probability of getting N 'up's in a row is $(1/2)^N$. The same goes for a local maximum. Any of the $2^N - 2$ other combinations of curvatures will result in a saddle point.

<ImageThemeAdjuster
  src="/images/blog/dimensions/dimensionality_curves.png"
  alt="Dimensionality Curves"
  strategy="invert"
  aspectRatio="16/9"
  className="w-full my-6"
  showCaption={true}
  caption="Dimensionality curves for the probability of a critical point being a local minimum/maximum, or saddle point."
/>

As the number of dimensions $N$ grows, the chances of landing on a true local minimum or maximum become exponentially small. The landscape becomes overwhelmingly dominated by saddle points.<MarginNote label="GOE Statistics">If we drop the independence assumption and use the actual GOE statistics for the Hessian, the all-positive probability shrinks even faster, like $$\exp(-\mathrm{const}\,N^{2})$$ in the unconstrained case and $$\exp(-\mathrm{const}\,N)$$ once we condition on hitting a critical point, but the key point survives: it’s still exponentially tiny in $$N$$ <Citation author="Fjodorov et al." year="2007" title="The density of stationary points in a high-dimensional random energy landscape and the onset of glassy behaviour" url="https://arxiv.org/pdf/cond-mat/0611585" venue="Methods of Theoretical Physics" /> <Citation author="Dean and Majumdar" year="2008" title="Extreme Value Statistics of Eigenvalues of Gaussian Random Matrices" url="https://arxiv.org/pdf/0801.1730" venue="Physical Review E" />. More on this in the next section.</MarginNote>

While this is a simplified picture, it gives us the right intuition. Now, let's make this more rigorous.

## The Geometry of Critical Points - why I cheer every extra parameter

To understand why high-dimensional optimization is different, we need to look beyond the gradient. When the gradient is zero, $\nabla f(w_t) = 0$, we're at a **critical point**. This could be a local minimum (a valley), a local maximum (a peak), or, most commonly, a saddle point.

To distinguish between them, we need the **Hessian matrix**-the matrix of second-order partial derivatives. For a function with $N$ parameters, the Hessian is an $N \times N$ matrix $H$ where each entry $H_{ij}$ is $\frac{\partial^2 f}{\partial w_i \partial w_j}$.

<center>$$ H = \begin{pmatrix}
\frac{\partial^2 f}{\partial w_1^2} & \frac{\partial^2 f}{\partial w_1 \partial w_2} & \cdots & \frac{\partial^2 f}{\partial w_1 \partial w_N} \\
\frac{\partial^2 f}{\partial w_2 \partial w_1} & \frac{\partial^2 f}{\partial w_2^2} & \cdots & \frac{\partial^2 f}{\partial w_2 \partial w_N} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial w_N \partial w_1} & \frac{\partial^2 f}{\partial w_N \partial w_2} & \cdots & \frac{\partial^2 f}{\partial w_N^2}
\end{pmatrix}$$</center>

The Hessian describes the local curvature of the loss landscape. The **eigenvalues** of this matrix tell us everything we need to know about the geometry of the critical point:

-   **Local Minimum**: All $N$ eigenvalues are positive. The surface curves upwards in every direction.
-   **Local Maximum**: All $N$ eigenvalues are negative. The surface curves downwards in every direction.
-   **Saddle Point**: A mix of positive and negative eigenvalues. The surface curves up in some directions and down in others.




Now, here's where it gets interesting. We earlier made a simple probabilistic argument. For a critical point in an $N$-dimensional space, we have $N$ eigenvalues. Each principal curvature is a coin flip, heads = "curves up", tails = "curves down". Flip **N** times; the odds of seeing all heads is $$(\tfrac12)^N$$.  That already smells exponential.


It's actually gnarlier than that, so bear with me.  

First gotcha: *those coins are rigged*. In a true random matrix the eigenvalues repel; large-deviation theory tells us the chance an $$N\times N$$ GOE matrix lands entirely on the positive side plummets like  

<center>$$ \Pr[\text{GOE p.d.}]\;\sim\;e^{-\beta N^{2}/4}, $$</center>

far worse than the innocent coin-flip would suggest <Citation author="Dean and Majumdar" year="2008" title="Extreme Value Statistics of Eigenvalues of Gaussian Random Matrices" url="https://arxiv.org/pdf/0801.1730" venue="Physical Review E" />.
But wait—there’s a reprieve.  If we condition on **actually** hitting a critical point, the Hessian distribution shifts just enough that the exponent drops from $$N^{2}$$ to $$N$$.  
Bray & Dean (2006) <Citation author="Bray and Dean" year="2006" title="The statistics of critical points of Gaussian fields on large-dimensional spaces" url="https://arxiv.org/abs/cond-mat/0611023" venue="Physical Review Letters" /> ran the full Kac–Rice integral and found

<center>$$ \Pr[\text{minimum}\mid\nabla L=0] \propto e^{-cN}. $$</center>

Minima are still exponentially rare, just not *spectacularly* rare. This reconciles us with the naïve argument, luckily enough.


If there’s any randomness at all in the loss landscape, which of course there is, it is thus vanishingly unlikely that all of the millions or billions of directions the model has to choose from will be simultaneously uphill. With so many directions to choose from you will always have at least one direction to escape. It’s just completely implausible that any big model comes close to any optima at all. In fact it’s implausible that an optimum exists. Unless you have a loss function that has a finite minimum value like squared loss (not cross entropy or softmax), or without explicit regularization that bounds the magnitude of the values, forces positive curvature, all real models *should* diverge. 
***

## The Over-parameterization Twist

That last part feels right, doesn't it? It’s a powerful, intuitive argument. It explains why we don’t get stuck. But it also leads to a slightly terrifying conclusion: if local minima are exponentially rare, and our models are basically just saddle-point-escaping machines, then are they ever *actually* finding a solution? Are they just wandering forever through a landscape with no bottom?

The argument I just made is true, but it's only half the story. It describes the world of a *generic, unstructured* high-dimensional function. But a neural network's loss function is anything but generic. It has structure. And that structure makes all the difference.

This brings us to the quote I started with:

> In high dimensional spaces, local minima are rare, but common in high-dimensional parametrizations of low-dimensional spaces.

Okay, hold on. If minima are so rare, why do our networks, which we keep making bigger and bigger, seem to find good solutions with relative ease? The very act of *over-parameterizing*—using far more parameters than are strictly necessary to solve a problem—alters the geometry of the loss landscape.

Let's get this straight. We aren't minimizing an arbitrary function $f(w)$ where $w \in \mathbb{R}^N$. We're minimizing a function that has a very particular structure. The loss typically depends on the *output* of the model. Let's call the model's output (its prediction on the training data) $z$. This output $z$ is produced by the parameters $w$ through some function $g$, so $z=g(w)$. The loss is then a function of this output, $L(z) = L(g(w))$.

Here’s the kicker: real deep-net losses *aren’t* unstructured; they’re high-dimensional *wrappers* around a low-dimensional task.  
Here’s the talisman I keep on my whiteboard:

<center>$$L(\theta)=\ell\bigl(g(\theta)\bigr), \qquad  \theta\!\in\!\mathbb R^{N},\; z=g(\theta)\!\in\!\mathbb R^{m},\; N\!\gg\!m. $$</center> 

The space of parameters $w$ is enormous (millions or billions of dimensions, $N$), but the space of the task variable $z$ is much smaller (e.g., the number of predictions on our training set, $m$). We have $N \gg m$.

This means the mapping from parameters to output, $g(w)$, is a *many-to-one* function. There are vast sets of different parameter configurations $w$ that produce the exact same output $z$ and therefore have the exact same loss value.

Geometrically, this creates "level sets" or "fibres" in the parameter space, huge $(N-m)$-dimensional surfaces where every point on the surface is equivalent in terms of the loss. The optimization is effectively happening on the much smaller $m$-dimensional "task landscape," but the path is being traced in the enormous $N$-dimensional parameter space.

## The Hessian, Revisited

So how does this structure affect our Hessian and the nature of critical points?

When we calculate the Hessian of $L$ with respect to the parameters $w$, the chain rule gives us a highly structured result. At a critical point (where the gradient is zero), the Hessian is no longer a wild, untamed $N \times N$ random matrix. It takes on a very specific, degenerate form:

<center>$$ H_w = J_g^\top H_z J_g $$</center>

Here, $J_g$ is the Jacobian of our parameter-to-output map $g$, and $H_z$ is the Hessian of the loss with respect to the output $z$. $H_z$ is a small $m \times m$ matrix, while $J_g$ is an $m \times N$ matrix.

This is the punchline. The matrix $H_w$ is an $N \times N$ matrix, but its **rank can be at most $m$**. An $N \times N$ matrix with rank $m$ has at least **$N-m$ zero eigenvalues**.

Let that sink in.

Those $N-m$ zero eigenvalues correspond to directions in the parameter space where you can move without changing the curvature of the loss. These are the flat directions along the fibres we just talked about.

Now our probabilistic argument about eigenvalues gets a massive update. The stability of a critical point is no longer decided by $N$ independent coin flips. It's decided by the $m$ eigenvalues of the much smaller, "active" Hessian $H_z$. The other $N-m$ directions are already flat (zero eigenvalues), which is perfectly compatible with being at a local minimum.

The probability that a critical point is a local minimum is now dictated by the signs of just $m$ eigenvalues.

<center>$$ \Pr[\text{minimum}\mid\text{critical}] \approx (0.5)^m $$</center>

This probability is **independent of $N$**, the number of parameters!

As we over-parameterize the model by making $N$ larger and larger, the probability of a critical point being a minimum *doesn't* vanish. It stays constant, depending only on the intrinsic dimensionality of the task, $m$.

And since the *total number* of critical points *does* increase with $N$, the result is an exponential *explosion* in the number of local minima. The landscape becomes filled with vast, interconnected valleys of solutions.

<MarginNote label="The Valley Floor">This is why we talk about "wide" versus "sharp" minima. The $N-m$ flat directions mean that any minimum we find isn't a single point, but part of a continuous manifold of other solutions. SGD with its inherent noise prefers to settle in these wide, flat valleys, which are often associated with better generalization.</MarginNote>

So, the paradox resolves itself beautifully.

1.  In a generic high-dimensional space, true local minima are exponentially rare. Gradient descent mostly has to navigate a sea of saddle points.
2.  But neural network loss landscapes are not generic. They are highly structured parametrizations of a low-dimensional task.
3.  This structure guarantees that the Hessian has a massive null-space, creating vast flat valleys. It makes local minima not only possible but *abundant*.

The "blessing of dimensionality" is really a two-part story. First, high dimensionality helps gradient descent easily escape saddle points. Second, the *structure* of over-parameterization fills that same high-dimensional space with wide, stable valleys of minima for gradient descent to find. The simple algorithm from the 19th century thrives because the landscapes we ask it to explore in the 21st are, against all intuition, perfectly suited for it.



